name: Build Executable

on: [push, pull_request]

jobs:
  pyinstaller-build:
    runs-on: windows-latest
    steps:
      - name: Check out the repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          pip uninstall numpy -y
          pip install -r requirements.txt
          python -c "import numpy; print('numpy version:', numpy.__version__)"

      - name: Install pyinstaller
        run: |
          $Env:PYINSTALLER_COMPILE_BOOTLOADER = "true"
          pip install --force-reinstall --ignore-installed --no-binary :all: pyinstaller==6.10.0

      - name: Install llamacpp with OpenBLAS support
        run: |
          pip install --upgrade pip setuptools
          pip uninstall -y llama-cpp-python
          $Env:CMAKE_ARGS = "-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS"
          pip install --force-reinstall --ignore-installed --no-cache-dir llama-cpp-python==0.2.90 numpy==1.25.2

      - name: Determine paths for dependencies
        run: |
          $ONNXRUNTIME_DLL = python -c 'import os, onnxruntime; print(os.path.join(os.path.dirname(onnxruntime.__file__), "capi", "onnxruntime_providers_shared.dll"))'
          $LLAMACPP_DLL = python -c 'import os, llama_cpp; print(os.path.join(os.path.dirname(llama_cpp.__file__), "lib", "llama.dll"))'
          echo "ONNXRUNTIME_DLL=$ONNXRUNTIME_DLL" >> $env:GITHUB_ENV
          echo "LLAMACPP_DLL=$LLAMACPP_DLL" >> $env:GITHUB_ENV
          python -c 'import onnxruntime; print("onnxruntime path:", onnxruntime.__file__)'
          python -c 'import llama_cpp; print("llama_cpp path:", llama_cpp.__file__)'

      - name: Debug environment variables
        run: |
          echo "ONNXRUNTIME_DLL=${{ env.ONNXRUNTIME_DLL }}"
          echo "LLAMACPP_DLL=${{ env.LLAMACPP_DLL }}"

      - name: Create Executable for AIServer
        run: |
          pyinstaller .\src\AIServer.py -y --onedir --clean --console --hidden-import=comtypes.stream --add-binary ${{ env.ONNXRUNTIME_DLL }}:. --add-binary ${{ env.LLAMACPP_DLL }}:.\llama_cpp\lib

      - name: Get current commit ID
        id: get_commit
        run: |
          $COMMIT_ID = git rev-parse HEAD
          echo "COMMIT_ID=$COMMIT_ID" >> $GITHUB_ENV

      - name: Upload build artifact
        uses: actions/upload-artifact@v4
        with:
          name: AIServer_v${{ steps.get_commit.outputs.COMMIT_ID }}
          path: ./dist
          compression-level: 0 # no compression

      - name: Add msbuild to PATH
        uses: microsoft/setup-msbuild@v2

      - name: Install Vulkan SDK
        uses: humbletim/install-vulkan-sdk@v1.1.1
        with:
          version: 1.3.261.1
          cache: true

      - name: Install llamacpp with Vulkan support
        run: |
          pip install --upgrade pip setuptools
          pip uninstall -y llama-cpp-python
          $Env:CMAKE_ARGS = "-DGGML_VULKAN=on"
          pip install --force-reinstall --ignore-installed --no-cache-dir llama-cpp-python==0.2.90 numpy==1.25.2

      - name: Create Executable for AIServer with llamacpp[vulkan]
        run: |
          pyinstaller .\src\AIServer.py -y --onedir --clean --console --hidden-import=comtypes.stream --add-binary ${{ env.ONNXRUNTIME_DLL }}:. --add-binary ${{ env.LLAMACPP_DLL }}:.\llama_cpp\lib

      - name: Upload build artifact
        uses: actions/upload-artifact@v4
        with:
          name: AIServer_v${{ steps.get_commit.outputs.COMMIT_ID }}-vulkan
          path: ./dist
          compression-level: 0 # no compression
